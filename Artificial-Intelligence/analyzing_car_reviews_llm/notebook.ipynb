{"cells":[{"cell_type":"markdown","id":"9aabafca-8129-4943-b865-d5e897637253","metadata":{},"source":["![image](data/car.jpeg)\n","\n","**Car-ing is sharing**, an auto dealership company for car sales and rental, is taking their services to the next level thanks to **Large Language Models (LLMs)**.\n","\n","As their newly recruited AI and NLP developer, you've been asked to prototype a chatbot app with multiple functionalities that not only assist customers but also provide support to human agents in the company.\n","\n","The solution should receive textual prompts and use a variety of pre-trained Hugging Face LLMs to respond to a series of tasks, e.g. classifying the sentiment in a carâ€™s text review, answering a customer question, summarizing or translating text, etc.\n","\n","\n","Project from DataCamp\n","\n","Author: Akash Kharita (akharita1999@gmail.com)"]},{"cell_type":"code","execution_count":2,"id":"5325a4c0-ceb3-4b66-acd2-5eadcefe3a63","metadata":{"collapsed":false,"executionCancelledAt":null,"executionTime":7878,"jupyter":{"outputs_hidden":false,"source_hidden":false},"lastExecutedAt":1746623155148,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import necessary packages\nimport pandas as pd\nimport torch\n\nfrom transformers import logging\nlogging.set_verbosity(logging.WARNING)\n\n# load HF library\nfrom transformers import pipeline","outputsMetadata":{"0":{"height":416,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["2025-05-07 13:05:51.941872: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1746623151.955976    4710 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1746623151.960401    4710 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","W0000 00:00:1746623151.972357    4710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1746623151.972370    4710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1746623151.972372    4710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","W0000 00:00:1746623151.972374    4710 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","2025-05-07 13:05:51.976642: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"]}],"source":["# Import necessary packages\n","import pandas as pd\n","import torch\n","\n","from transformers import logging\n","logging.set_verbosity(logging.WARNING)\n","\n","# load HF library\n","from transformers import pipeline"]},{"cell_type":"markdown","id":"24cb0387-7e1c-468b-b706-2b9e21d49ffc","metadata":{},"source":["# Task 1. Sentiment-Classification\n","\n","Understand the sentiment of a review whether it is positive or negative"]},{"cell_type":"markdown","id":"c5af976f-13b1-4442-b638-f711f9bfe380","metadata":{},"source":["## 1.1 Load Car Review Data\n","We begin by loading the dataset from car_reviews.csv, which contains customer reviews along with their associated sentiment (class) labels. Each entry in the file is separated by a semicolon (;), so we use that as the delimiter when reading the data."]},{"cell_type":"code","execution_count":3,"id":"6c3a3bf7-8675-4fdf-8580-0c69b58f471e","metadata":{"executionCancelledAt":null,"executionTime":53,"lastExecutedAt":1746623167399,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# reading the dataframe\ndf = pd.read_csv(\"data/car_reviews.csv\", delimiter = ';')\n\n# printing first few columns to check the structure. \nprint(df.columns)\nprint(df.head())","outputsMetadata":{"0":{"height":164,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Index(['Review', 'Class'], dtype='object')\n","                                              Review     Class\n","0  I am very satisfied with my 2014 Nissan NV SL....  POSITIVE\n","1  The car is fine. It's a bit loud and not very ...  NEGATIVE\n","2  My first foreign car. Love it, I would buy ano...  POSITIVE\n","3  I've come across numerous reviews praising the...  NEGATIVE\n","4  I've been dreaming of owning an SUV for quite ...  POSITIVE\n"]}],"source":["# reading the dataframe\n","df = pd.read_csv(\"data/car_reviews.csv\", delimiter = ';')\n","\n","# printing first few columns to check the structure. \n","print(df.columns)\n","print(df.head())"]},{"cell_type":"markdown","id":"63c65665-00c2-4e03-8abf-5cf60c1346ca","metadata":{},"source":["## 1.2 Run a pre-trained sentiment classifier on the reviews\n","\n","\n","### Loading the pre-trained model\n","\n","In this section, we will use a pre-trained transformer model to perform sentiment analysis on the car reviews. We will utilize the `transformers` library to load a sentiment analysis pipeline and apply it to our dataset.\n","\n","### What is `pipeline`?\n","\n","A `pipeline` is a module that helps with\n"," - loading,\n"," - preprocessing,\n"," - tokenizing,\n"," - predicting\n"," - converting the output into human friendly.\n","\n","for any deep learning model. its an umbrella resource for all. \n","\n","\n","So for any task and model the format is  \n","`pipeline(task = 'sentiment-analysis', model = 'distilbert-base-uncased-finetuned-sst-2-english')`\n","\n","### What is model here?\n","Let's understand more about the model used here \n","\n","distilbert -  A lighter version of BERT, an encoding only model typically for tasks like sentiment-analysis. \n","base - Model size - base = medium (not small, not large)\n","uncased - Does not differentiate upper/lower cases (Love = LOVE)\n","finetuned-sst-2 - This model was trained on SST-2 dataset, Stanford Sentiment Treebank v2\n","english - language of training model. "]},{"cell_type":"code","execution_count":4,"id":"2daa8a76-89b2-426a-8a02-3205070a04bd","metadata":{"executionCancelledAt":null,"executionTime":1271,"lastExecutedAt":1746623172018,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"classifier = pipeline(task = 'sentiment-analysis', model = 'distilbert-base-uncased-finetuned-sst-2-english')","outputsMetadata":{"4":{"height":38,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1934970e1fbd4d059b3f147fb3234128","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e332a81c9ec443d09da36b0a8691ecea","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5ab7fb655b9046baad1162f4b79279fb","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8179ed81198f4b839d90a04f2f29f79c","version_major":2,"version_minor":0},"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cpu\n"]}],"source":["classifier = pipeline(task = 'sentiment-analysis', model = 'distilbert-base-uncased-finetuned-sst-2-english')"]},{"cell_type":"markdown","id":"3fbff45a-cc21-4e78-8f90-9ceefc334313","metadata":{},"source":["## 1.3 Classifying the input. \n","\n","One may ask where is the tokenization step? \n","\n","when we specify classifier(input), then pipeline internally tokenizes and the token depends on the model being used. \n","Usually the process is tokenize+pad+truncate+model forward pass + decode. \n","\n","There situations where we may need to tokenize first, but not in this case, here we can directly use the forward pass of the model on the input. \n","\n","Note that the classifier takes the list of strings (or sentences) as input.\n","\n","### 1.3.1 Explanation of the output of the function\n","The output of the `classifier(input)` contains two things  - labels (self explanatory) and score. This score reflects model's confidence in its predictions, its actually the output of the softmax prob function. "]},{"cell_type":"code","execution_count":5,"id":"e3bd4918-66b1-433f-bf7f-52785b83cf30","metadata":{"executionCancelledAt":null,"executionTime":262,"lastExecutedAt":1746623176106,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Extract data\ninput = df['Review'].values.tolist()\n\n# Predict sentiments\npredicted_labels  = classifier(input)\nprint(predicted_labels)","outputsMetadata":{"0":{"height":80,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[{'label': 'POSITIVE', 'score': 0.929397702217102}, {'label': 'POSITIVE', 'score': 0.8654273152351379}, {'label': 'POSITIVE', 'score': 0.9994640946388245}, {'label': 'NEGATIVE', 'score': 0.9935314059257507}, {'label': 'POSITIVE', 'score': 0.9986565113067627}]\n"]}],"source":["# Extract data\n","input = df['Review'].values.tolist()\n","\n","# Predict sentiments\n","predicted_labels  = classifier(input)\n","print(predicted_labels)"]},{"cell_type":"markdown","id":"c670a2e7-290a-4281-9c7d-df88754bc776","metadata":{},"source":["## 1.4 Evaluate the model performance\n","\n","### 1.4.1 Convert predictions to binary labels {0,1}\n","\n","Ok so the outputs are labels and scores. If we want to compute performance metrics such as accuracy or F1, we need to first convert the categorical labels into numerical. We will also need to convert the true labels into numerical format. "]},{"cell_type":"code","execution_count":6,"id":"58e992d0-d915-4c66-91b9-02ca338d5ce7","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1746623180149,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Map labels to 0 (NEGATIVE) or 1 (POSITIVE)\npredictions = [1 if label['label'] == 'POSITIVE' else 0 for label in predicted_labels]\nprint(predictions)\n","outputsMetadata":{"0":{"height":38,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["[1, 1, 1, 0, 1]\n"]}],"source":["# Map labels to 0 (NEGATIVE) or 1 (POSITIVE)\n","predictions = [1 if label['label'] == 'POSITIVE' else 0 for label in predicted_labels]\n","print(predictions)\n"]},{"cell_type":"markdown","id":"acac334e-3551-4304-a78e-ee3a507de64d","metadata":{},"source":["### 1.4.2 Prepare true labels for evaluation\n","\n","Note that we need to convert it into list because predictions and references should be list of sentences or list of strings. "]},{"cell_type":"code","execution_count":7,"id":"c3c31200-2725-46e5-98fc-4a36f3439f14","metadata":{"executionCancelledAt":null,"executionTime":9,"lastExecutedAt":1746623186742,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# True binary labels from the CSV (assuming 'POSITIVE' and 'NEGATIVE')\ntrue_labels = df['Class'].head(5).map({'POSITIVE': 1, 'NEGATIVE': 0}).tolist()\n"},"outputs":[],"source":["# True binary labels from the CSV (assuming 'POSITIVE' and 'NEGATIVE')\n","true_labels = df['Class'].head(5).map({'POSITIVE': 1, 'NEGATIVE': 0}).tolist()\n"]},{"cell_type":"markdown","id":"cc9af76f-b256-4feb-85d7-30fdfcc69476","metadata":{},"source":["### 1.4.3 Calculate Accuracy & F1 score. \n","\n","We will be using the HF's evaluate library. Evaluate library contains all the necessary metrics. "]},{"cell_type":"code","execution_count":8,"id":"4ce6fed2-ed3d-4ff1-86ff-1c5e7370928f","metadata":{"executionCancelledAt":null,"executionTime":741,"lastExecutedAt":1746623192290,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import evaluate\n\n# Load metrics\naccuracy = evaluate.load(\"accuracy\")\nf1 = evaluate.load(\"f1\")\n\n# Compute\naccuracy_result = accuracy.compute(predictions=predictions, references=true_labels)\nf1_result = f1.compute(predictions=predictions, references=true_labels)\n\n# Print results\nprint(\"Accuracy:\", accuracy_result['accuracy'])\nprint(\"F1 Score:\", f1_result['f1'])\n","outputsMetadata":{"2":{"height":59,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3a8c72ec040d4d38a2a6eff8904dca72","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"220f68796d4a49508b9f5fdc2e2b4429","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/6.79k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.8\n","F1 Score: 0.8571428571428571\n"]}],"source":["import evaluate\n","\n","# Load metrics\n","accuracy = evaluate.load(\"accuracy\")\n","f1 = evaluate.load(\"f1\")\n","\n","# Compute\n","accuracy_result = accuracy.compute(predictions=predictions, references=true_labels)\n","f1_result = f1.compute(predictions=predictions, references=true_labels)\n","\n","# Print results\n","print(\"Accuracy:\", accuracy_result['accuracy'])\n","print(\"F1 Score:\", f1_result['f1'])\n"]},{"cell_type":"markdown","id":"df97e31a-dd6d-4e97-a633-b9cbb3b59560","metadata":{},"source":["# Task 2: Language Translation: English to Spanish\n","\n","![image](data/translation.webp)\n","\n","Due to computational restrictions, we are just going to take the first two sentences of the first review and translate it into spanish using our LLM. "]},{"cell_type":"markdown","id":"a21e007d-1811-41a1-bb49-5e3a84240f59","metadata":{},"source":["## 2.1 Load the data\n","\n","Before that we have to import some necessary libaries. \n","NLTK = Natural Language Toolkit\n","\n","Its a trusted library that is used a lot in NLP, research and even in production\n","\n","punkt is a model that identifies the boundaries between sentences. "]},{"cell_type":"markdown","id":"c2ab9e2d-29c6-46ef-925c-1a4e0b5e4bd6","metadata":{},"source":["## 2.2 Preparing the input \n","We will extract the first two sentence from the first review and then join them. "]},{"cell_type":"code","execution_count":9,"id":"c06b3452-cf3a-40af-9374-5e126f439cde","metadata":{"executionCancelledAt":null,"executionTime":47,"lastExecutedAt":1746623200695,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Get the first review\nfirst_review = df['Review'][0]\n\n# Split into sentences\nsentences = first_review.split('.')[0:2]\n\n# join the sentences to create input\ninput_text = \". \".join(sentences) + \".\"\n\n# Take the first 2 sentences for translation\nprint(\"Original (English):\", input_text)","outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Original (English): I am very satisfied with my 2014 Nissan NV SL.  I use this van for my business deliveries and personal use.\n"]}],"source":["# Get the first review\n","first_review = df['Review'][0]\n","\n","# Split into sentences\n","sentences = first_review.split('.')[0:2]\n","\n","# join the sentences to create input\n","input_text = \". \".join(sentences) + \".\"\n","\n","# Take the first 2 sentences for translation\n","print(\"Original (English):\", input_text)"]},{"cell_type":"markdown","id":"ba628909-17f6-4a91-9286-09792e003f20","metadata":{},"source":["## 2.3 Translating the input text\n","\n","Just a bit of context about the model that is used here - \n","Helsinki-NLP : Research group at University of Helsinki (Finland)\n","opus-mt : Large multilingual project. \n","en-to-es: english to espaniol\n","\n"]},{"cell_type":"code","execution_count":10,"id":"d7126305-5646-46ad-8987-9dd76bc07bfa","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"},"1":{"height":59,"type":"stream"},"8":{"height":38,"type":"stream"},"9":{"height":59,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3f5a6d07fa414758b161bcfe9a82aca9","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.47k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71728d1809dd4433b411c5a10ee07792","version_major":2,"version_minor":0},"text/plain":["pytorch_model.bin:   0%|          | 0.00/312M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4729c55edbc34cba8c8a7ea19bc45548","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/312M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b08c834048e843be88974593876be355","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/293 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c63e4c2e402048e883a0ccf36ff0def2","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/44.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"43ce72c7648c4665ad42f86e1ea0e1d4","version_major":2,"version_minor":0},"text/plain":["source.spm:   0%|          | 0.00/802k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"93bf8f3780f744ab91ff98ffc901c1d9","version_major":2,"version_minor":0},"text/plain":["target.spm:   0%|          | 0.00/826k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71cab25534ea4c5babeaa8b15ec2920f","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/1.59M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cpu\n"]},{"name":"stdout","output_type":"stream","text":["Translated (Spanish): Estoy muy satisfecho con mi Nissan NV SL 2014. Uso esta camioneta para mis entregas de negocios y uso personal.\n"]}],"source":["translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-en-es\")\n","translated_output = translator(input_text)\n","translated_review = translated_output[0]['translation_text']\n","print(\"Translated (Spanish):\", translated_review)"]},{"cell_type":"markdown","id":"9bf076e6-45ac-43f6-9e69-99d8ce32f80b","metadata":{},"source":["### 2.3.1 Reading the ground truth"]},{"cell_type":"code","execution_count":11,"id":"6f9299aa-4d1d-499c-9e37-cfb72448c425","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":59,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Estoy muy satisfecho con mi Nissan NV SL 2014. Utilizo esta camioneta para mis entregas comerciales y uso personal.\n"]}],"source":["# Load from .txt file\n","with open(\"data/reference_translations.txt\", \"r\", encoding=\"utf-8\") as f:\n","    reference_translation = f.readline().strip()\n","\n","print(reference_translation)"]},{"cell_type":"markdown","id":"cf9d9e7f-a407-45a5-babf-c828fa6943c8","metadata":{},"source":["## 2.4 Evaluating the results\n","\n","\n","\n","Note the following is very important in case of LLMs. \n","BLEU is commonly used to evaluate LLM translation. For single sentence comparison BLEU expects \n","- predictions - list of strings (one string per sentence)\n","- references  - list of list of strings (each inner list = all possible correct translations)"]},{"cell_type":"code","execution_count":12,"id":"9224689e-c67d-4da3-988a-09a48f503a9e","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":38,"type":"stream"},"3":{"height":38,"type":"stream"}}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f16ea076707d40d3a896f1ed4774360f","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"407346914c104deea66f7cbbac8d9841","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cacb7e66355841eca8fcafd77adf886d","version_major":2,"version_minor":0},"text/plain":["Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["BLEU Score: 0.6888074582865503\n"]}],"source":["# Convert to BLEU format (list of list of tokens)\n","references = [[reference_translation]]\n","predictions = [translated_review]\n","\n","\n","bleu = evaluate.load(\"bleu\")\n","bleu_score = bleu.compute(predictions= predictions, references= references)\n","print(\"BLEU Score:\", bleu_score['bleu'])"]},{"cell_type":"markdown","id":"4e3f5e38-f8b3-49d6-9ed3-d9247d742266","metadata":{},"source":["# Task 3: Extracting QA \n","\n","![image](data/qanda.jpg)\n","\n","Answering a question \"What de he like about the brand?\" based on the second review. \n","\n","Ok so we are using a minilm-uncased-squad2 LLM here. \n","\n","- deepset : The organization that trained and released the model (known for Haystack QA framework)\n","- minilm : The base model - a super lightweight, fast version of BERT (great for low latency)\n","- uncased : The model ignores capitalization (treats BMW and bmw as same)\n","- squad2 : Trained on SQUAD 2.0 dataset. "]},{"cell_type":"code","execution_count":14,"id":"433177c7-5108-497b-ae08-a4f1f279c9e7","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":206,"type":"stream"},"2":{"height":185,"type":"stream"},"6":{"height":38,"type":"stream"},"7":{"height":206,"type":"stream"}}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at deepset/minilm-uncased-squad2 were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Device set to use cpu\n"]}],"source":["# Load the extractive QA model\n","qa_pipeline = pipeline(task = \"question-answering\", model = \"deepset/minilm-uncased-squad2\")\n","\n","# Load your dataset\n","second_review = df['Review'].iloc[1]\n","\n","# Define question and context\n","question = \"What did he like about the brand?\"\n","context = second_review"]},{"cell_type":"markdown","id":"f393a707-0a31-4eb0-aa0c-36b7fc38f40d","metadata":{},"source":["## 3.1 Run the QA model\n","\n","The output contains following - \n","\n","- score : A confidence score between 0 and 1, that indicates the model confidence that this is a best answer.\n","- start & end: The position in the context where the answer is extracted.\n","- answer: The output answer. "]},{"cell_type":"code","execution_count":15,"id":"095c4056-2dca-4f7f-8900-c7ac922e1bd8","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":206,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["Question: What did he like about the brand?\n","Context: The car is fine. It's a bit loud and not very powerful. On one hand, compared to its peers, the interior is well-built. The transmission failed a few years ago, and the dealer replaced it under warranty with no issues. Now, about 60k miles later, the transmission is failing again. It sounds like a truck, and the issues are well-documented. The dealer tells me it is normal, refusing to do anything to resolve the issue. After owning the car for 4 years, there are many other vehicles I would purchase over this one. Initially, I really liked what the brand is about: ride quality, reliability, etc. But I will not purchase another one. Despite these concerns, I must say, the level of comfort in the car has always been satisfactory, but not worth the rest of issues found.\n","Answer: ride quality, reliability\n"]}],"source":["# Run the QA model\n","output = qa_pipeline(question=question, context=context)\n","\n","# Extract the answer\n","answer = output['answer']\n","\n","# Print result\n","print(f\"Question: {question}\")\n","print(f\"Context: {context}\")\n","print(f\"Answer: {answer}\")"]},{"cell_type":"markdown","id":"743d1541-a2d5-41cd-b260-8118a57d367b","metadata":{},"source":["# Task 4: Summarization \n","\n","![image](data/summary.jpg)\n","\n","Summarizing the last review of the dataset into approximately 50-55 tokens. \n","\n"]},{"cell_type":"markdown","id":"74b57fe4-e121-45ab-9b40-da4d74febf3c","metadata":{},"source":["## 4.1 Loading the dataset. "]},{"cell_type":"code","execution_count":19,"id":"ec715a2d-5298-46af-a4ff-c02beee77b5f","metadata":{"executionCancelledAt":null,"executionTime":88,"lastExecutedAt":1746624917737,"lastExecutedByKernel":"7f1d83bd-cf0d-4408-8266-2f622f15502b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"last_review = df['Review'].iloc[-1]\nprint(last_review)","outputsMetadata":{"0":{"height":227,"type":"stream"}}},"outputs":[{"name":"stdout","output_type":"stream","text":["I've been dreaming of owning an SUV for quite a while, but I've been driving cars that were already paid for during an extended period. I ultimately made the decision to transition to a brand-new car, which, of course, involved taking on new payments. However, given that I don't drive extensively, I was inclined to avoid a substantial financial commitment. The Nissan Rogue provides me with the desired SUV experience without burdening me with an exorbitant payment; the financial arrangement is quite reasonable. Handling and styling are great; I have hauled 12 bags of mulch in the back with the seats down and could have held more. I am VERY satisfied overall. I find myself needing to exercise extra caution when making lane changes, particularly owing to the blind spots resulting from the small side windows situated towards the rear of the vehicle. To address this concern, I am actively engaged in making adjustments to my mirrors and consciously reducing the frequency of lane changes. The engine delivers strong performance, and the ride is really smooth.\n"]}],"source":["last_review = df['Review'].iloc[-1]\n","print(last_review)"]},{"cell_type":"markdown","id":"75d6dacf-11dd-4fd9-9889-b3a5f35f863b","metadata":{},"source":["## 4.2 Load the summarization pipeline. \n","\n","Note that do_sample brings the creativity, if its false, then the model just outputs similar to inputs without any randomization, it will summarize what the input said without any modifications. \n","\n","Another Note, the output of a summarizer is always a list and hence summary[0]"]},{"cell_type":"code","execution_count":null,"id":"58b58730-065b-4928-b0fd-17937a8d4a52","metadata":{},"outputs":[],"source":["# Load summarization\n","\n","summarizer = pipeline(\"summarization\", model = \"facebook/bart-large-cnn\")\n","\n","# Run summarization (aiming for ~ 5- -55 tokens)\n","summary = summarizer(last_review, max_length = 55, min_length = 45, do_sample = False)\n","\n","\n","# Extract text\n","summarized_text = summary[0]['summary_text']\n","\n","# Print result\n","print(summarized_text)\n"]}],"metadata":{"editor":"DataLab","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":5}
