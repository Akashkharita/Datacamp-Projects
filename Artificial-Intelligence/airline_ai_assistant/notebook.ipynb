{"cells":[{"source":"![](./cover.jpg)\n\nYour airline's customer service team has been collecting chat data for years—thousands of conversations, each labeled with the user’s intent and an ideal response. Now, it's time to put that data to work.\n\nYou've been tasked with fine-tuning a TinyLlama model to power the airline’s next-gen AI assistant. The goal? Given a user message, the model should predict the intent (like booking a flight, checking baggage status, or requesting special assistance) and generate a helpful, human-like response. Accurate intent detection is key since it helps the system understand what the customer wants, so it can respond appropriately and trigger downstream actions when needed.\n\n### The Data\nYou'll work with a dataset of various travel query examples. \n\n Column | Description |\n|--------|-------------|\n| ```instruction``` | A user request from the Travel domain |\n| ```category``` | The high-level semantic category for the intent |\n| ```intent``` | The specific intent corresponding to the user instruction |\n| ```response``` | An example of an expected response from the virtual assistant |\n\n___\n### Update to Python 3.10\n\nDue to how frequently the libraries required for this project are updated, you'll need to update your environment to Python 3.10:\n\n1. In the workbook, click on \"Environment,\" in the top toolbar and select \"Session details\".\n\n2. In the workbook language dropdown, select \"Python 3.10\".\n\n3. Click \"Confirm\" and hit \"Done\" once the session is ready.","metadata":{},"id":"40386fc5-3663-4cca-a90b-4a414e75a229","cell_type":"markdown"},{"source":"# First install the necessary packages\n!pip install -q -q -q trl==0.16.0\n!pip install -q -q -q tf-keras==2.19.0\n!pip install -q -q -q peft==0.14.0","metadata":{"executionCancelledAt":null,"executionTime":9170,"lastExecutedAt":1747875354192,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# First install the necessary packages\n!pip install -q -q -q trl==0.16.0\n!pip install -q -q -q tf-keras==2.19.0\n!pip install -q -q -q peft==0.14.0","outputsMetadata":{"0":{"height":332,"type":"stream"}}},"id":"de19a3c2-640a-4783-8cde-5a95e88d9bc2","cell_type":"code","execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"}]},{"source":"# Import the required dependencies for this project\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\n\nfrom datasets import Dataset, load_dataset\nfrom collections import Counter, defaultdict\nimport random","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1747875354243,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Import the required dependencies for this project\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom trl import SFTTrainer, SFTConfig\nfrom peft import LoraConfig\n\nfrom datasets import Dataset, load_dataset\nfrom collections import Counter, defaultdict\nimport random","outputsMetadata":{"0":{"height":416,"type":"stream"}}},"id":"14082b66-5845-496b-888a-ebf28e4927a6","cell_type":"code","execution_count":22,"outputs":[]},{"source":"The code below loads the travel query dataset and reduces it from ~30k to ~50 records, keeping all intent types. This speeds up fine-tuning. Run it before starting, and feel free to experiment with it later!","metadata":{},"id":"d295fc56-0302-41a3-a1be-a863547fdef4","cell_type":"markdown"},{"source":"# First load the entire dataset\nds = load_dataset('bitext/Bitext-travel-llm-chatbot-training-dataset', split=\"train\")\n\n# Group examples by intent\nrandom.seed(42)\nintent_groups = defaultdict(list)\nfor record in ds:\n    intent = record[\"intent\"]\n    intent_groups[intent].append(record)\n\n# Determine how many samples per intent\ntotal_intents = len(intent_groups)\nsamples_per_intent = 100 // total_intents\n\n# Sample from each intent\nbalanced_subset = []\nfor intent, examples in intent_groups.items():\n    sampled = random.sample(examples, min(samples_per_intent, len(examples)))\n    balanced_subset.extend(sampled)\n\ntotal_num_of_records = 50    \ntravel_chat_ds = Dataset.from_list(balanced_subset[:total_num_of_records])\n\ntravel_chat_ds.to_pandas().head(3)","metadata":{"executionCancelledAt":null,"executionTime":1755,"lastExecutedAt":1747875355998,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# First load the entire dataset\nds = load_dataset('bitext/Bitext-travel-llm-chatbot-training-dataset', split=\"train\")\n\n# Group examples by intent\nrandom.seed(42)\nintent_groups = defaultdict(list)\nfor record in ds:\n    intent = record[\"intent\"]\n    intent_groups[intent].append(record)\n\n# Determine how many samples per intent\ntotal_intents = len(intent_groups)\nsamples_per_intent = 100 // total_intents\n\n# Sample from each intent\nbalanced_subset = []\nfor intent, examples in intent_groups.items():\n    sampled = random.sample(examples, min(samples_per_intent, len(examples)))\n    balanced_subset.extend(sampled)\n\ntotal_num_of_records = 50    \ntravel_chat_ds = Dataset.from_list(balanced_subset[:total_num_of_records])\n\ntravel_chat_ds.to_pandas().head(3)","outputsMetadata":{"0":{"height":501,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"f62c2997-7fde-4538-b6f9-ff8797ebb5ba","nodeType":"const"}}},"1":{"height":50,"type":"dataFrame","tableState":{"customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"7da20b7d-655a-4080-87b0-f429ec0cf6a0","nodeType":"const"},"quickFilterText":""}},"3":{"height":500,"type":"dataFrame","tableState":{}},"4":{"height":50,"type":"dataFrame","tableState":{"quickFilterText":"","customFilter":{"const":{"type":"boolean","valid":true,"value":true},"id":"7da20b7d-655a-4080-87b0-f429ec0cf6a0","nodeType":"const"}}}}},"id":"daa57ab5-d87c-42e6-808e-1e048ae3e0ad","cell_type":"code","execution_count":23,"outputs":[{"output_type":"execute_result","data":{"application/com.datacamp.data-table.v2+json":{"table":{"schema":{"fields":[{"name":"index","type":"integer"},{"name":"instruction","type":"string"},{"name":"intent","type":"string"},{"name":"category","type":"string"},{"name":"tags","type":"string"},{"name":"response","type":"string"}],"primaryKey":["index"],"pandas_version":"1.4.0"},"data":{"index":[0,1,2],"instruction":["I'd like information about my checked baggage allowance, how can I find it?","i have to see the fucking checked baggage allowance can i get some help","I want to know about my checked baggage allowance, where can I find more information?"],"intent":["check_baggage_allowance","check_baggage_allowance","check_baggage_allowance"],"category":["BAGGAGE","BAGGAGE","BAGGAGE"],"tags":["BCIP","BCIQW","BCI"],"response":["To retrieve your checked baggage allowance details, please follow these instructions:\n\n1. Visit {{WEBSITE_URL}} or launch the {{APP_NAME}} application.\n2. Log in to your personal account.\n3. Select the {{BOOKINGS_OPTION}} section.\n4. Enter the required booking or flight information.\n5. The specific baggage allowance for your trip will be displayed.\n\nShould you require additional help, do not hesitate to contact customer support via the {{APP_NAME}} app or on {{WEBSITE_URL}}.","To determine your checked baggage allowance, please follow these instructions: \n\n1. Go to {{WEBSITE_URL}} or launch the {{APP_NAME}} app. \n2. Log in to your account. \n3. Select the {{BOOKINGS_OPTION}} section. \n4. Enter the relevant booking or flight details. \n5. You will then see your checked baggage allowance information. \n\nIf you need further help, contact our customer service via the {{APP_NAME}} app or visit {{WEBSITE_URL}} for more support.","To find details regarding your checked baggage allowance, please follow these steps:\n\n1. Visit {{WEBSITE_URL}} or access the {{APP_NAME}} application.\n2. Log into your personal account.\n3. Select the {{BOOKINGS_OPTION}} section.\n4. Enter your booking reference or flight information.\n5. Your baggage allowance will be displayed accordingly.\n\nFor additional help, reach out to our customer support team available via the {{APP_NAME}} app or on {{WEBSITE_URL}}."]}},"total_rows":3,"truncation_type":null},"text/plain":"                                         instruction  ...                                           response\n0  I'd like information about my checked baggage ...  ...  To retrieve your checked baggage allowance det...\n1  i have to see the fucking checked baggage allo...  ...  To determine your checked baggage allowance, p...\n2  I want to know about my checked baggage allowa...  ...  To find details regarding your checked baggage...\n\n[3 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>instruction</th>\n      <th>intent</th>\n      <th>category</th>\n      <th>tags</th>\n      <th>response</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>I'd like information about my checked baggage ...</td>\n      <td>check_baggage_allowance</td>\n      <td>BAGGAGE</td>\n      <td>BCIP</td>\n      <td>To retrieve your checked baggage allowance det...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>i have to see the fucking checked baggage allo...</td>\n      <td>check_baggage_allowance</td>\n      <td>BAGGAGE</td>\n      <td>BCIQW</td>\n      <td>To determine your checked baggage allowance, p...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>I want to know about my checked baggage allowa...</td>\n      <td>check_baggage_allowance</td>\n      <td>BAGGAGE</td>\n      <td>BCI</td>\n      <td>To find details regarding your checked baggage...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{"application/com.datacamp.data-table.v2+json":{"status":"success"}},"execution_count":23}]},{"source":"# Start the project with the dataset below\n\nprint(travel_chat_ds)","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1747875356047,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start the project with the dataset below\n\nprint(travel_chat_ds)","outputsMetadata":{"0":{"height":101,"type":"stream"}}},"id":"7daaca1d-0c39-4ed5-8631-a61c30f00f36","cell_type":"code","execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":"Dataset({\n    features: ['instruction', 'intent', 'category', 'tags', 'response'],\n    num_rows: 50\n})\n"}]},{"source":"# Start coding here\n# Use as many cells as you need","metadata":{"executionCancelledAt":null,"executionTime":49,"lastExecutedAt":1747875356096,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Start coding here\n# Use as many cells as you need"},"id":"07446785-915c-472d-971e-132f08ab9371","cell_type":"code","execution_count":25,"outputs":[]},{"source":"# Modify the dataset to include a conversation field\ndef merge_example(row):    \n  row['conversation'] = f\"Query: {row['instruction']}\\nIntent: {row['intent']}\\nResponse: {row['response']}\"\n  return row\n\ntravel_chat_ds = travel_chat_ds.map(merge_example)\n\nprint(travel_chat_ds[0]['conversation'])","metadata":{"executionCancelledAt":null,"executionTime":64,"lastExecutedAt":1747875356160,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Modify the dataset to include a conversation field\ndef merge_example(row):    \n  row['conversation'] = f\"Query: {row['instruction']}\\nIntent: {row['intent']}\\nResponse: {row['response']}\"\n  return row\n\ntravel_chat_ds = travel_chat_ds.map(merge_example)\n\nprint(travel_chat_ds[0]['conversation'])","outputsMetadata":{"1":{"height":269,"type":"stream"}}},"cell_type":"code","id":"f1956429-a900-405b-a70c-84a881ab0e2b","outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19629a3de112461ca36a16b6693020c7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":"Query: I'd like information about my checked baggage allowance, how can I find it?\nIntent: check_baggage_allowance\nResponse: To retrieve your checked baggage allowance details, please follow these instructions:\n\n1. Visit {{WEBSITE_URL}} or launch the {{APP_NAME}} application.\n2. Log in to your personal account.\n3. Select the {{BOOKINGS_OPTION}} section.\n4. Enter the required booking or flight information.\n5. The specific baggage allowance for your trip will be displayed.\n\nShould you require additional help, do not hesitate to contact customer support via the {{APP_NAME}} app or on {{WEBSITE_URL}}.\n"}],"execution_count":26},{"source":"# 1. Load the model and tokenizer\nIn this section, we load the pre-trained TinyLlama model and its tokenizer \nfrom Hugging Face Hub. This model is a compact, causal language model \ndesigned for lightweight inference and experimentation.","metadata":{},"cell_type":"markdown","id":"348baa3b-087f-4ce0-bdda-4b7b898ea555"},{"source":"# Load the Llama Model\nmodel_name=\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n","metadata":{"executionCancelledAt":null,"executionTime":344,"lastExecutedAt":1747875356504,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Load the Llama Model\nmodel_name=\"TinyLlama/TinyLlama-1.1B-Chat-v0.1\"\nmodel = AutoModelForCausalLM.from_pretrained(model_name)\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ntokenizer.pad_token = tokenizer.eos_token\n"},"cell_type":"code","id":"19bdd29d-ae76-44b5-9000-ebaab3fa03a2","outputs":[],"execution_count":27},{"source":"# 2. Setting up the training Arguments\nThis block defines the training configuration for supervised fine-tuning (SFT)\nusing Hugging Face's TRL (Transformers Reinforcement Learning) library.\n\nSFTConfig allows us to control how the model learns during fine-tuning.\nWe set conservative values here to allow quick experimentation and avoid\nresource bottlenecks.\n\nKey Parameters:\n- max_steps: Limits total training steps. Set to 1 for a quick test run.\n- per_device_train_batch_size: Batch size per device (GPU/CPU).\n- gradient_accumulation_steps: Number of steps to accumulate gradients before an optimizer update.\n- learning_rate: Step size for weight updates.\n- max_grad_norm: Clips gradients to prevent explosion.\n- save_steps: Frequency at which the model is checkpointed.\n- dataset_text_field: Name of the column in your dataset containing the prompt or input text.\n- output_dir: Directory to save logs and model checkpoints.\n\"\"\"\n","metadata":{},"cell_type":"markdown","id":"2d47ea88-d7ce-4608-9dcc-3eadbe227a68"},{"source":"# Initialize SFTConfig\nsftConfig = SFTConfig(\n    max_steps=1,    \n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-3,\n    max_grad_norm=0.3,\n    save_steps=100,\n    dataset_text_field='conversation',\n    output_dir=\"/tmp\",\n)","metadata":{"executionCancelledAt":null,"executionTime":44,"lastExecutedAt":1747875356550,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize SFTConfig\nsftConfig = SFTConfig(\n    max_steps=1,    \n    per_device_train_batch_size=1,\n    gradient_accumulation_steps=1,\n    learning_rate=2e-3,\n    max_grad_norm=0.3,\n    save_steps=100,\n    dataset_text_field='conversation',\n    output_dir=\"/tmp\",\n)"},"cell_type":"code","id":"309bc18d-e275-4b31-b03f-752112eea90a","outputs":[],"execution_count":28},{"source":"# 3 Initialize LoRA (Low-Rank Adaptation) Configuration\n\n\"\"\"\nThis configuration sets up Low-Rank Adaptation (LoRA) for parameter-efficient fine-tuning \nof large language models. Instead of updating the full weight matrices of the model, \nLoRA injects small trainable matrices into attention layers — significantly reducing \nthe number of trainable parameters.\n\nKey Parameters:\n- r: The rank of the low-rank matrices A and B (controls model capacity and memory usage).\n- lora_alpha: Scaling factor for the LoRA update. Controls how much the LoRA adaptation influences the base model.\n- lora_dropout: Dropout applied on the LoRA layers to improve generalization.\n- bias: Whether to train bias terms. \"none\" disables it to keep things lightweight.\n- task_type: Specifies the type of model. For causal language modeling (GPT-style), use \"CAUSAL_LM\".\n- target_modules: List of model modules (typically attention projections) where LoRA will be applied.\n\"\"\"","metadata":{},"cell_type":"markdown","id":"b819d279-fac4-439e-af18-66ad71dc3b28"},{"source":"# Initialize LoRA config\nlora_config = LoraConfig(    \n    r= 4,    \n    lora_alpha= 16,    \n    lora_dropout=0.05,    \n    bias=\"none\",    \n    task_type=\"CAUSAL_LM\",    \n    target_modules=['q_proj', 'v_proj']\n)","metadata":{"executionCancelledAt":null,"executionTime":22,"lastExecutedAt":1747875356572,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize LoRA config\nlora_config = LoraConfig(    \n    r= 4,    \n    lora_alpha= 16,    \n    lora_dropout=0.05,    \n    bias=\"none\",    \n    task_type=\"CAUSAL_LM\",    \n    target_modules=['q_proj', 'v_proj']\n)"},"cell_type":"code","id":"c9edfd79-ee33-4efd-9605-32462e5a4ec7","outputs":[],"execution_count":29},{"source":"# 4 Initialize the SFTTrainer for Fine-Tuning\n\n\"\"\"\nThis block initializes the `SFTTrainer`, a high-level training loop provided by Hugging Face's \n`trl` library for supervised fine-tuning (SFT) of language models using parameter-efficient \nmethods like LoRA.\n\nThe `SFTTrainer` handles tokenization, batching, gradient accumulation, checkpointing, and \nLoRA integration under the hood — allowing for quick experimentation without boilerplate.\n\nParameters:\n- model: The pre-trained causal language model to be fine-tuned (TinyLlama in this case).\n- train_dataset: The dataset containing input prompts (and optionally responses).\n- peft_config: The LoRA configuration that determines which parts of the model are adapted.\n- args: Training arguments specified via `SFTConfig` (e.g., learning rate, batch size, max steps).\n\"\"\"\n\nfrom trl import SFTTrainer\n\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=travel_chat_ds,   # Hugging Face Dataset object\n    peft_config=lora_config,        # LoRA config defined above\n    args=sftConfig                  # Supervised fine-tuning config\n)\n\nprint(\"✅ SFTTrainer initialized and ready to train.\")\n","metadata":{},"cell_type":"markdown","id":"22193781-317d-49a3-990d-a18d8fb1ad8c"},{"source":"# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=travel_chat_ds,\n    peft_config=lora_config,\n    args=sftConfig\n)","metadata":{"executionCancelledAt":null,"executionTime":349,"lastExecutedAt":1747875356921,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Initialize SFTTrainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=travel_chat_ds,\n    peft_config=lora_config,\n    args=sftConfig\n)"},"cell_type":"code","id":"eab817ff-5b71-4eb3-83f5-ec3438e796fa","outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33a35b1d704340169f324715ead7dd3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"090b89b1b468496db52637e4eb633a4d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad023ff23899408bb99dbcc22975308f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f7cea1889124529a2a6b7ea9fdd028b"}},"metadata":{}}],"execution_count":30},{"source":"# 5 Start Fine-Tuning the Model\n\n\nThis command starts the supervised fine-tuning process using the `SFTTrainer`.\n\nWhat happens under the hood:\n- The model parameters (as specified by the LoRA configuration) are updated using the training data.\n- If `max_steps` is set (e.g., `max_steps=1`), training will terminate after that number of optimization steps.\n- Intermediate metrics like loss, speed, and gradient stats may be logged, depending on `logging_steps` and `save_steps`.\n- The model will be saved in `output_dir` if save checkpoints are triggered.\n\nThis is where the model begins learning domain-specific behavior — in this case, for travel-related customer support.\n","metadata":{},"cell_type":"markdown","id":"2aa23768-0cb4-4d02-af1a-61425852433c"},{"source":"# Kickstart fine-tuning process\ntrainer.train()","metadata":{"executionCancelledAt":null,"executionTime":54149,"lastExecutedAt":1747875411072,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Kickstart fine-tuning process\ntrainer.train()"},"cell_type":"code","id":"7175001a-f9aa-4dbc-876a-98a556777f3f","outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2/1 : < :, Epoch 0.02/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1, training_loss=2.135936975479126, metrics={'train_runtime': 52.4515, 'train_samples_per_second': 0.019, 'train_steps_per_second': 0.019, 'total_flos': 1372512940032.0, 'train_loss': 2.135936975479126})"},"metadata":{},"execution_count":31}],"execution_count":31},{"source":"# 6 Generate a Response with the Fine-Tuned Model\n\n\"\"\"\nNow that the model is fine-tuned, we can prompt it with a user query and \ngenerate a response using the `generate()` method.\n\nSteps:\n1. Tokenize the input prompt using the model's tokenizer.\n2. Use the `.generate()` method to produce a continuation from the model.\n3. Decode the generated tokens (ignoring special tokens) to get a readable string.\n\"\"\"","metadata":{},"cell_type":"markdown","id":"81a3ae8f-5df6-4812-9451-3a012a5b84de"},{"source":"# Generate responses with fine-tuned model\ninputs = tokenizer.encode(\"Query: I'm trying to book a flight\", return_tensors=\"pt\")\noutputs = model.generate(\n    inputs,\n    max_new_tokens=20,\n    do_sample=False,\n    num_beams=1,\n    early_stopping=True\n)\n\ndecoded_outputs = tokenizer.decode(outputs[0, inputs.shape[1]:], skip_special_tokens=True)\nmodel_response = decoded_outputs\n","metadata":{"executionCancelledAt":null,"executionTime":null,"lastExecutedAt":null,"lastExecutedByKernel":null,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":null,"outputsMetadata":{"0":{"height":80,"type":"stream"}}},"cell_type":"code","id":"ec3f97a8-77d0-475e-b6c4-d932a8d9e8cd","outputs":[],"execution_count":19},{"source":"model_response","metadata":{"executionCancelledAt":null,"executionTime":10,"lastExecutedAt":1747875336816,"lastExecutedByKernel":"f45f756c-e716-4173-9733-44470d69992b","lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"model_response"},"cell_type":"code","id":"05dd3366-a78b-4ef1-8240-5e4516e26454","outputs":[{"output_type":"execute_result","data":{"text/plain":"\"from London Gatwick to New York JFK. I've found 3 ways to\""},"metadata":{},"execution_count":20}],"execution_count":20}],"metadata":{"colab":{"name":"Welcome to DataCamp Workspaces.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"editor":"DataLab"},"nbformat":4,"nbformat_minor":5}